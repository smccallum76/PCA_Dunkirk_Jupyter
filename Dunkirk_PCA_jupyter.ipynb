{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d2ed229",
   "metadata": {},
   "source": [
    "# Principal Components Analysis: Dunkirk XRF Data\n",
    "\n",
    "Code snippets showing the basic steps used to build the Dunkirk PCA dashboard including:\n",
    "* Data import\n",
    "* Visual data inspection via box plots by element and element/lithology\n",
    "* Application of PCA using NumPy\n",
    "* Application of PCA using sklearn\n",
    "* PCA interpretation via scatter plots and bar plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99c4ba7",
   "metadata": {},
   "source": [
    "## Data Import\n",
    "For this example, the data is stored in csv files, but the web-based dashboard uses a postgreSQL database for all data.  \n",
    "\n",
    "The ```data``` file includes the corrected XRD elemental data with all elemental measurements reported in ppm.  The ```meta``` file contains location and lithology specific information about each sample.  The 'data' and the 'meta' file are merged into a data frame (df) that will be used for subsequent analytics.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objects as go\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "# imports\n",
    "data = pd.read_csv('data_chemostrat_benchtop_compiled.csv')\n",
    "meta = pd.read_csv('info_chemostrat_benchtop_compiled.csv')\n",
    "data.drop(columns=['Sample ID'], inplace=True) # drop a repeat column before merge\n",
    "# merge data and metadata\n",
    "df = data.merge(meta, on='key')\n",
    "# check the head of the df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba919486",
   "metadata": {},
   "source": [
    "## Visually inspect the data (box plots)\n",
    "The modules below will be used to define an element list and generate a box plot and lithology specific plots for visual inspection of the data. The box plot is used to identify unusual data points that warrant further investigation.  It is also helpful for gaining a preliminary understanding of how the data are distributed. \n",
    "\n",
    "The code block below contains 3 primary components:\n",
    "* **Component 1**: Definition of 3 lists that are used for both plotting and dropdown menus\n",
    "* **Component 2**: Definition of the application (```app```) layout.  The layout defined in this Jupyter notebook a slightly more streamlined than that of the actual dashboard as only one ```div``` is used.\n",
    "* **Component 3**: Definition of ```callback``` function allowing for updating the box plots according to the user input. \n",
    "\n",
    "The above components, specifically component 2 and 3 will be repeated in subsequent code blocks but will not be called out as was done in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d059a0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --------- COMPONENT 1 ---------\n",
    "elem = [column for column in df.columns if len(column)<=2] # list of elements\n",
    "yvals=[300000, 100000, 10000, 1000, 100, 50] # y-axis values in ppm that are used to rescale the range on the box plot\n",
    "ylab = ['300K ppm', '100K ppm', '10K ppm', '1K ppm', '100 ppm', '50 ppm'] # y-axis labels that the user will see\n",
    "\n",
    "# --------- COMPONENT 2 ---------\n",
    "app = JupyterDash(__name__) # initiate the app\n",
    "\n",
    "# define layout containing a dropdown menu and a box plot\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        html.H3('Select a y-axis max value:'),\n",
    "        dcc.Dropdown(\n",
    "        id='yaxis_dropdown',\n",
    "        options=[{'label': ylab[i], 'value': yvals[i]}\n",
    "                for i in range(len(yvals))\n",
    "                ],\n",
    "        value=300000,\n",
    "        multi=False\n",
    "                )\n",
    "        ], style={'width':'20%'}),  \n",
    "    # box plots for elements and elements by lithology (defined in callback function)\n",
    "    dcc.Graph(id='box')\n",
    "])\n",
    "\n",
    "# --------- COMPONENT 3 ---------\n",
    "# callback function that updates the box plot to honor changes to the y-axis range. \n",
    "@app.callback(\n",
    "    Output('box', 'figure'),\n",
    "    [Input('yaxis_dropdown', 'value')])\n",
    "def update_BoxWhisker(value): \n",
    "    # the metrics are every parameter that the user has selected \n",
    "    boxplot = {'data':[\n",
    "                        go.Box(y=df[e], name=e, boxmean='sd',\n",
    "                               boxpoints='outliers',\n",
    "                               # outliers are defined as 1.5 +/- IQR per plotly \n",
    "                               marker={'color':'lightseagreen'},\n",
    "                               text=df['Lithology']) for e in elem],\n",
    "                            \n",
    "                            'layout':go.Layout(title='Elemental Data Box Plots',\n",
    "                                               xaxis={'title':'Elements'},\n",
    "                                               yaxis={'title':'ppm', 'type':'linear',\n",
    "                                                      'range':[0,value]},\n",
    "                                               annotations=[dict(x=20, y=value*0.8, \n",
    "                                                                 text='Sample Count = ' + str(len(df)), \n",
    "                                                                 showarrow=False)])\n",
    "                }\n",
    "    return boxplot\n",
    "\n",
    "app.run_server(mode='inline', port=8050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75278b73",
   "metadata": {},
   "source": [
    "### Box plots grouped by lithology\n",
    "The approach below is like that shown above, but now the differences in elemental distributions as a function of lithology can be observed.  This visualization is helpful for understanding the some of the data points that appear to be unusual but are in fact a reflection of lithologic changes. \n",
    "\n",
    "Note that ```app``` is defined again in this module, but this is not the case in the web-based dashboard.  Redefining ```app``` and ```app.run_server``` are necessary for the Jupyter notebook examples only.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87e9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = JupyterDash(__name__) # initiate the app\n",
    "# define the layout\n",
    "app.layout = html.Div([\n",
    "                html.Div([\n",
    "                    html.H5('Select an Element:'),\n",
    "                    dcc.Dropdown(\n",
    "                        id='element_dropdown',\n",
    "                        options=[{'label':e, 'value':e} for e in elem],\n",
    "                        value='Al',\n",
    "                        multi=False\n",
    "                                )\n",
    "                        ], style={'width':'20%'}),\n",
    "                    dcc.Graph(id='box-lith')\n",
    "\n",
    "                    ])\n",
    "\n",
    "# define the callback function\n",
    "@app.callback(\n",
    "    Output('box-lith', 'figure'),\n",
    "    [Input('element_dropdown', 'value')])\n",
    "def update_BoxWhiskerLith(value): \n",
    "    \n",
    "    # the metrics are every parameter that the user has selected \n",
    "    boxLith_plot = {'data':[\n",
    "                        go.Box(y=df[df['Lithology']=='Grey Shale'][value], \n",
    "                               boxmean='sd',\n",
    "                               boxpoints='outliers',\n",
    "                               # outliers are defined as 1.5 +/- IQR per plotly \n",
    "                               marker={'color':'grey'},\n",
    "                               name='Grey Shale' ),\n",
    "                        \n",
    "                        go.Box(y=df[df['Lithology']=='Black Shale'][value], \n",
    "                               boxmean='sd',\n",
    "                               boxpoints='outliers',\n",
    "                               # outliers are defined as 1.5 +/- IQR per plotly \n",
    "                               marker={'color':'black'},\n",
    "                               name='Black Shale' ),\n",
    "                        \n",
    "                        go.Box(y=df[df['Lithology']=='Black Shale/pyrite'][value], \n",
    "                               boxmean='sd',\n",
    "                               boxpoints='outliers',\n",
    "                               # outliers are defined as 1.5 +/- IQR per plotly \n",
    "                               marker={'color':'gold'},\n",
    "                               name='Black Shale/Pyrite' )],\n",
    "                            \n",
    "                            'layout':go.Layout(title='Elemental Data Box Plots Grouped by Lithology',\n",
    "                                               xaxis={'title':str(value)},\n",
    "                                               yaxis={'title':'ppm', 'type':'linear'})\n",
    "                                               }\n",
    "    return boxLith_plot\n",
    "\n",
    "app.run_server(mode='inline', port=8060)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef81034",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (PCA)\n",
    "\n",
    "For illustrative purposes, two methods for applying PCA have been included below.  The first method uses _NumPy_ only to implement PCA and the second uses _sklearn_ for a more direct application.  Both methods are applied to a toy data set followed by application of the sklearn method to the standardized XRF data.  \n",
    "\n",
    "The basic steps for applying PCA are as follows:\n",
    "1. Review data and identify any data that may warrant further investigation\n",
    "2. Standardize or normalize the data.\n",
    "    * **Standardize**: mean of zero and standard deviation of 1 (standardization was used here)\n",
    "    * **Normalize**: scale each input to have max value 1 and min of 0 (or some constant range)\n",
    "3. Calculate covariance matrix\n",
    "4. Extract eigenvalues and eigenvectors from covariance matrix\n",
    "    * **eigenvalues**: magnitude of the eigenvector and explain the amount of variance explained by a given eigenvector.  The higher the eigenvalue the more of the overall variance that is captured the respective eigenvector.  \n",
    "    * **eigenvector**: direction of variance as defined by contributions from each variable.  The eigenvectors are the principal components.  \n",
    "5. Rank the principal components from most to least variance explained.  This is done by using the eigenvalues, where the highest eigenvalue corresponds to PC1, the second highest to PC2, and so on. \n",
    "6. Determine the number of principal components to retain, which will be project specific.  Typically, the number of components necessary to account for >80% of the overall variance is used as a rule of thumb. \n",
    "\n",
    "When using _sklearn_, steps 3, 4, and 5 are handled in the background by the PCA class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cb886",
   "metadata": {},
   "source": [
    "### NumPy PCA, Toy Dataset (this is an aside to the XRF dataset)\n",
    "\n",
    "To illustrate the steps listed above, a toy dataset will be used.  The toy dataset is taken from **_Statistics and Data Analysis in Geology, Davis, pg. 509_**.  The _NumPy_ output will be compared to the _sklearn_ output.  \n",
    "\n",
    "**Note**: The standardization step is skipped in this example because this dataset contains only two variables with values in the same units of measurement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [3,4,6,6,6,7,7,8,9,9,9,10,11,12,12,13,13,13,13,14,15,17,17,18,20] # variable 1\n",
    "x2 = [2,10,5,8,10,2,13,9,5,8,14,7,12,10,11,6,14,15,17,7,13,13,17,19,20] # variable 2\n",
    "x = np.array([x1,x2]) # convert x1 and x2 into a matrix\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0218d9",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ece05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure() # initiate a figure\n",
    "# add the scatter plot\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x[0,:], y=x[1,:],\n",
    "    mode='markers')\n",
    "    )\n",
    "# add chart and axis titles\n",
    "fig.update_layout(title='x1 vs x2 Scatter Plot',\n",
    "                 xaxis={'title':'x1'},\n",
    "                 yaxis={'title':'x2'},\n",
    "                 autosize=False,\n",
    "                 width=750,\n",
    "                 height=750)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12d722",
   "metadata": {},
   "source": [
    "Generate the covariance matrix from x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(x) # covariance matrix\n",
    "cov_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae594c",
   "metadata": {},
   "source": [
    "Extract the eigenvalue (w) and the eigenvector (v) from the covariance matrix (s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_eigenvalues,numpy_eigenvectors = np.linalg.eig(cov_matrix)\n",
    "print('Eigenvectors: \\n', numpy_eigenvectors, '\\n')\n",
    "print('Eigenvalues: \\n', numpy_eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126741ae",
   "metadata": {},
   "source": [
    "Sort the **eigenvectors** from greatest to least **eigenvalue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947eb31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.vstack((numpy_eigenvectors,numpy_eigenvalues)).T # use a temp array to stitch the eigenvectors and eigenvalues together\n",
    "temp_sort = temp[np.argsort(-temp[:, 2])] # sort the array by eigenvalues descending order\n",
    "numpy_eigenvectors_sort = temp_sort[:,0:2] # sorted eigenvectors by descending eigenvalue\n",
    "numpy_eigenvalues_sort = temp_sort[:,2] # sorted eigenvalues, descending order\n",
    "\n",
    "print('Sorted Eigenvectors: \\n', numpy_eigenvectors_sort, '\\n', 'NOTE: each column is an eigenvector', '\\n')\n",
    "print('Sorted Eigenvalues: ', '\\n', numpy_eigenvalues_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a561ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting point of eigenvectors 1 and 2 (i.e., PC1 and PC2) for illustrative purposes\n",
    "evec1 = numpy_eigenvectors_sort[:,0] * numpy_eigenvalues_sort[0]*-1 # coordinates of eigenvector 1 (PC1)\n",
    "evec2 = numpy_eigenvectors_sort[:,1] * numpy_eigenvalues_sort[1]*-1 # coordinates of eigenvector 2 (PC2)\n",
    "# multiplication by -1 is for plotting convinience only\n",
    "evec1 = np.vstack(([0,0], evec1))\n",
    "evec2 = np.vstack(([0,0], evec2))\n",
    "print(evec1)\n",
    "\n",
    "fig = go.Figure() # initiate a figure\n",
    "# add the scatter plot\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x[0,:] , y=x[1,:],\n",
    "    mode='markers',\n",
    "    name='data')\n",
    "    )\n",
    "fig.add_trace(go.Scatter( # multiply x an y by 0.5 so half the variance is in in quad 1 and the other half in quad 3\n",
    "    x=evec1[:,0]*0.5 + np.mean(x[0,:]), y=evec1[:,1]*0.5 + np.mean(x[1,:]), name='PC1_quadrant1', mode='lines', marker={'color':'red'}\n",
    "    ))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=evec1[:,0]*(-0.5) + np.mean(x[0,:]), y=evec1[:,1]*(-0.5) + np.mean(x[1,:]), name='PC1_quadrant3', mode='lines', marker={'color':'red'},\n",
    "    line = dict(shape = 'linear', color = 'red', width= 2, dash = 'dash')\n",
    "    ))\n",
    "\n",
    "fig.add_trace(go.Scatter( # multiply x and y by 0.5 so half the variance is in quad 2 and the other half in quad 4\n",
    "    x=evec2[:,0]*(-0.5) + np.mean(x[0,:]), y=evec2[:,1]*(-0.5) + np.mean(x[1,:]), name='PC2_quadrant2', mode='lines', marker={'color':'black'}\n",
    "    ))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=evec2[:,0]*0.5 + np.mean(x[0,:]), y=evec2[:,1]*0.5 + np.mean(x[1,:]), name='PC2_quadrant4', mode='lines', marker={'color':'black'},\n",
    "    line = dict(shape = 'linear', color = 'black', width= 2, dash = 'dash')\n",
    "    ))\n",
    "# add chart and axis titles\n",
    "fig.update_layout(title='x1 vs x2 Scatter Plot with PC1 and PC2 Overlay',\n",
    "                 xaxis={'title':'x1', 'range':[-5,25]},\n",
    "                 yaxis={'title':'x2', 'range':[-5,25]},\n",
    "                 autosize=False,\n",
    "                 width=750, # use a fixed width and height so that PC1 and PC2 plot orthogonal to each other\n",
    "                 height=750)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9551a3a3",
   "metadata": {},
   "source": [
    "Amount of variance explained by each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde00df",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_total_variance = np.sum(numpy_eigenvalues_sort)\n",
    "numpy_pct_variance = numpy_eigenvalues_sort / numpy_total_variance * 100\n",
    "numpy_pct_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d4433",
   "metadata": {},
   "source": [
    "**PCA Score and Correlation**\n",
    "\n",
    "The PCA score is the result of the matrix multiplication of the input data with the eigenvectors.  PCA scores for each sample are calculated. The score shows a given sample's affinity for each principal component. For example, sample-1 will have a score for principal component 1 (PC_1) and principal component 2 (PC_2). A sample (row of data) that scores high for PC_1 indicates that this sample was enriched in elements that account for most of the variance in this component.  Samples with a low negative score for a given PC indicate that they vary inversely with the same PC.  \n",
    "\n",
    "After the PCA scores are calculated each score is correlated against the input data (```xt``` in this case).  The trend of the correlation will be identical to that of the eigenvectors (when the data is standardized). The correlation comparison helps with interpretation of each principal component (eigenvector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = x.T # transpose of input data for proper alignment in matrix multiplication\n",
    "numpy_pcaScore = np.matmul(xt, numpy_eigenvectors_sort) # matrix multiplication\n",
    "# correlation between input data, xt, and the PCA score\n",
    "numpy_pcaCorr = np.corrcoef(xt, numpy_pcaScore, rowvar=False) # correlation matrix\n",
    "numpy_pcaCorr = numpy_pcaCorr[0:len(numpy_eigenvectors_sort), len(numpy_eigenvectors_sort)::]*-1\n",
    "# convert to a dataframe for easier viewing\n",
    "numpy_pcaCorr_df = pd.DataFrame(numpy_pcaCorr, index=['x1', 'x2'], columns=['PC1', 'PC2'])\n",
    "numpy_pcaCorr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa1d5c2",
   "metadata": {},
   "source": [
    "**Bar Plots of eigenvectors and PCA score correlation**\n",
    "\n",
    "These plots are helpful in interpretation of the pca output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3786e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- EIGENVECTOR WEIGHTS BY VARIABLE X1, X2  ---------\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=['x1', 'x2'], y=numpy_eigenvectors_sort[:,0]*(-1),\n",
    "           marker={'color':'red'},\n",
    "           name='PC1'))\n",
    "fig.add_trace(go.Bar(x=['x1', 'x2'], y=numpy_eigenvectors_sort[:,1]*(-1),\n",
    "           marker={'color':'grey'},\n",
    "           name='PC2'))\n",
    "\n",
    "# update the layout\n",
    "fig.update_layout(title='Eigenvector Weights by Variable',\n",
    "                   xaxis={'title':'Variables'},\n",
    "                   yaxis={'title':'Eigenvector weight (normalized variance)'})\n",
    "fig.show()\n",
    "\n",
    "# --------- CORRELATION OF PCA SCORE VS VARIABLES X1, X2 ---------\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=['x1', 'x2'], y=numpy_pcaCorr[:,0],\n",
    "           marker={'color':'red'},\n",
    "           name='PC1'))\n",
    "fig.add_trace(go.Bar(x=['x1', 'x2'], y=numpy_pcaCorr[:,1],\n",
    "           marker={'color':'grey'},\n",
    "           name='PC2'))\n",
    "\n",
    "# update the layout\n",
    "fig.update_layout(title='Correlation of PCA Score vs Variables',\n",
    "                   xaxis={'title':'Variables'},\n",
    "                   yaxis={'title':'Correlation Coefficient'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab30ed",
   "metadata": {},
   "source": [
    "### Sklearn PCA, Toy Dataset (one more time)\n",
    "This example is identical to the _NumPy_ example above, except it will be much more compact since _sklearn_ is being used.  The print statements in the block below show direct comparisons of both the _NumPy_ and _sklearn_ instances of the PCA applied to the toy dataset. It is evident from the max difference comparisons that both methods are effectively identical.  Using _sklearn_ is a more compact method for applying PCA, but it is helpful to use the _NumPy_ method for better understanding of the PCA workflow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f59af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [3,4,6,6,6,7,7,8,9,9,9,10,11,12,12,13,13,13,13,14,15,17,17,18,20] # variable 1 (copy of data from NumPy example)\n",
    "x2 = [2,10,5,8,10,2,13,9,5,8,14,7,12,10,11,6,14,15,17,7,13,13,17,19,20] # variable 2 (copy of data from NumPy example)\n",
    "x = np.array([x1,x2]).T # convert x1 and x2 into a matrix (note that x has to be transposed prior to sklearn PCA)\n",
    "\n",
    "# the two lines below are the entire pca using sklearn\n",
    "pca = PCA(n_components=2) # instantiate a PCA\n",
    "sklearn_pca = pca.fit(x) # fit the PCA to the data (x)\n",
    "\n",
    "sklearn_eigenvectors = sklearn_pca.components_ # eigenvector attribute of sklearn_pca\n",
    "sklearn_eigenvalues = sklearn_pca.explained_variance_ # eigenvalue attribute of sklearn_pca\n",
    "sklearn_pctVar = sklearn_pca.explained_variance_ratio_ # percent of explained variance attribute of sklearn_pca\n",
    "\n",
    "# print output to compare sklearn and NumPy)\n",
    "print('Eigenvectors from sklearn: \\n', sklearn_eigenvectors, '\\n')\n",
    "print('Eigenvectors from NumPy: \\n', numpy_eigenvectors_sort*-1, '\\n')\n",
    "print('Eigenvalues from sklearn: \\n', sklearn_eigenvalues, '\\n')\n",
    "print('Eigenvalues from NumPy: \\n', numpy_eigenvalues_sort, '\\n')\n",
    "print('Percent variance from sklearn: ', sklearn_pctVar *100)\n",
    "print('Percent variance from NumPy: ', numpy_pct_variance, '\\n')\n",
    "print('Eigenvector max difference (sklearn vs NumPy): ', np.max(sklearn_eigenvectors - numpy_eigenvectors_sort*(-1)))\n",
    "print('Eigenvalue max difference (sklearn vs NumPy): ', np.max(sklearn_eigenvalues - numpy_eigenvalues_sort))\n",
    "print('Percent variance max difference (sklearn vs NumPy): ', np.max(sklearn_pctVar*100 - numpy_pct_variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4318f9c",
   "metadata": {},
   "source": [
    "## PCA applied to XRF Data\n",
    "In this section a PCA will be applied to standardized XRF data using _sklearn_. Not shown here, but the PCA using the non-standardized data is very helpful for understanding the elements that are most influential. \n",
    "\n",
    "**Step 1** Standardize the XRF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_stdz will be a numpy array containing the standardized XRF data.  \n",
    "dataStdz = StandardScaler().fit_transform(df[elem])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f167a",
   "metadata": {},
   "source": [
    "**Step 2** Use _sklearn_ PCA to define the PCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10359be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of components will be equal to the number of elements, but reduced in later steps\n",
    "pca_model = PCA(n_components=len(elem)) # 42 components (only 10 will be used)\n",
    "pca = pca_model.fit(dataStdz)                       # fit the model\n",
    "eigenvalues_dataStdz = pca.explained_variance_      # eigenvalues (magnitude of each principal component)\n",
    "eigenvectors_dataStdz = pca.components_             # eigenvectors (principal component vectorrs)\n",
    "explainVar_dataStdz = pca.explained_variance_ratio_ # fraction of variance explained by each principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7246f7",
   "metadata": {},
   "source": [
    "### Plot the PCA Output\n",
    "\n",
    "A total of 4 plots will be used to gain understanding of the PCA output:\n",
    "\n",
    "* **Plot 1**: Bar plot showing the variance contribution of each PC\n",
    "* **Plot 2**: 3D scatter plot showing the PC-scores colored by lithology\n",
    "* **Plot 3**: Bar plot showing each elements contribution to the variance of a given PC\n",
    "* **Plot 4**: Bar plot showing the correlation of each elements PC-score with the input elemental data \n",
    "\n",
    "**Plot 1**: Bar plot showing the variance contribution of each PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names for the bar chart\n",
    "pc_cols = ['pc_'+str(i+1) for i in range(len(eigenvectors_dataStdz))]\n",
    "# cumulative contribution to the variance\n",
    "cumVar_dataStdz = np.cumsum(explainVar_dataStdz)\n",
    "# generate the figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(x=pc_cols[0:10], y=explainVar_dataStdz[0:10],\n",
    "           marker={'color':'deeppink'},\n",
    "           name='Inc. Var.'))\n",
    "fig.add_trace(go.Bar(x=pc_cols[0:10], y=cumVar_dataStdz[0:10],\n",
    "           marker={'color':'grey'},\n",
    "           name='Cum. Var.'))\n",
    "# update the layout\n",
    "fig.update_layout(title='Percent Variance (cropped to first 10 components)',\n",
    "                   xaxis={'title':'Principal Components'},\n",
    "                   yaxis={'title':'Fraction of Variance Accounted For'})\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e7b11",
   "metadata": {},
   "source": [
    "**Plot 2**: 3D scatter plot showing the PC-scores colored by lithology\n",
    "\n",
    "Visualizing more than 3 dimensions is difficult, but the plot below allows for visualization of any combination of 3 principal components for PC 1-10.  It is evident from the plot that the first 3 PCs effectively differentiate the 3 primary lithologies described thus far (Black Shale/Pyrite, Black Shale, and Grey Shale). The lithology labels are based on field descriptions for each section for which samples were collected.  When viewing PC_1, PC_2, and PC_3 it is evident that the Black Shale/Pyrite lithology is characterized by elevated PC_1 scores, the Black Shale lithology by elevated PC_2 scores, and the Grey Shale lithology has the highest (and lowest) PC_3 scores, but is also low in PC_2 and PC_1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc781603",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate the PCA scores for the standardized xrf data\n",
    "dataStdz_scores = np.matmul(dataStdz, eigenvectors_dataStdz.T)\n",
    "# convert scores to dataframe\n",
    "dataStdz_scores_df = pd.DataFrame(dataStdz_scores, columns=pc_cols)\n",
    "# add lithology labels\n",
    "dataStdz_scores_df['lith'] = list(df['Lithology'])\n",
    "\n",
    "app = JupyterDash(__name__) # initiate the app\n",
    "\n",
    "app.layout = html.Div([\n",
    "       html.Div([\n",
    "        html.H5('X-Axis'),\n",
    "        dcc.Dropdown(\n",
    "            id='x_3d_stand_pca',\n",
    "            options=[{'label':'PC_'+str(i+1), 'value':'pc_'+str(i+1)} for i in range(10)],\n",
    "            value='pc_1',\n",
    "            multi=False)\n",
    "       ], style={'width':'20%', 'display':'inline-block'}),\n",
    "       html.Div([\n",
    "        html.H5('Y-Axis'),\n",
    "        dcc.Dropdown(\n",
    "            id='y_3d_stand_pca',\n",
    "            options=[{'label':'PC_'+str(i+1), 'value':'pc_'+str(i+1)} for i in range(10)],\n",
    "            value='pc_2',\n",
    "            multi=False),\n",
    "        ], style={'width':'20%', 'display':'inline-block', 'margin-left':'10px'}),\n",
    "        html.Div([\n",
    "        html.H5('Z-Axis'),\n",
    "        dcc.Dropdown(\n",
    "            id='z_3d_stand_pca',\n",
    "            options=[{'label':'PC_'+str(i+1), 'value':'pc_'+str(i+1)} for i in range(10)],\n",
    "            value='pc_3',\n",
    "            multi=False),\n",
    "       ], style={'width':'20%', 'display':'inline-block','margin-left':'10px'}),\n",
    "    \n",
    "        dcc.Graph(id='scatter3d')\n",
    "        ])\n",
    "@app.callback(\n",
    "    Output('scatter3d', 'figure'),\n",
    "    [Input('x_3d_stand_pca', 'value'),\n",
    "     Input('y_3d_stand_pca', 'value'),\n",
    "     Input('z_3d_stand_pca', 'value')]\n",
    "    )\n",
    "def update_3dScatter_pca(x_value, y_value, z_value):\n",
    "    \n",
    "    pca_scatter = {'data':[go.Scatter3d(\n",
    "                    x=dataStdz_scores_df[x_value][dataStdz_scores_df['lith']=='Grey Shale'],\n",
    "                    y=dataStdz_scores_df[y_value][dataStdz_scores_df['lith']=='Grey Shale'],\n",
    "                    z=dataStdz_scores_df[z_value][dataStdz_scores_df['lith']=='Grey Shale'],\n",
    "                    mode='markers',\n",
    "                    marker = {\n",
    "                        'size':8,\n",
    "                        'line':{'color':'white', 'width':3},\n",
    "                        'opacity':0.6,\n",
    "                        'color':'grey',\n",
    "                        },\n",
    "                    name='Grey Shale'),\n",
    "                    go.Scatter3d(\n",
    "                    x=dataStdz_scores_df[x_value][dataStdz_scores_df['lith']=='Black Shale'],\n",
    "                    y=dataStdz_scores_df[y_value][dataStdz_scores_df['lith']=='Black Shale'],\n",
    "                    z=dataStdz_scores_df[z_value][dataStdz_scores_df['lith']=='Black Shale'],\n",
    "                    mode='markers',\n",
    "                    marker = {\n",
    "                        'size':8,\n",
    "                        'line':{'color':'white', 'width':3},\n",
    "                        'opacity':0.6,\n",
    "                        'color':'black',\n",
    "                        },\n",
    "                    name='Black Shale'),\n",
    "                    go.Scatter3d(\n",
    "                    x=dataStdz_scores_df[x_value][dataStdz_scores_df['lith']=='Black Shale/pyrite'],\n",
    "                    y=dataStdz_scores_df[y_value][dataStdz_scores_df['lith']=='Black Shale/pyrite'],\n",
    "                    z=dataStdz_scores_df[z_value][dataStdz_scores_df['lith']=='Black Shale/pyrite'],\n",
    "                    mode='markers',\n",
    "                    marker = {\n",
    "                        'size':8,\n",
    "                        'line':{'color':'white', 'width':3},\n",
    "                        'opacity':0.6,\n",
    "                        'color':'gold',\n",
    "                        },\n",
    "                    name='Black Shale/Pyrite')],\n",
    "                    'layout':go.Layout(title='3D Scatterplot - PC Score (standardized data)',\n",
    "                                   height=500,\n",
    "                                   width=600,\n",
    "                                   scene={'xaxis':{'title':'X-'+x_value},\n",
    "                                          'yaxis':{'title':'Y-'+y_value},\n",
    "                                          'zaxis':{'title':'Z-'+z_value}})\n",
    "                        }\n",
    "    \n",
    "    return pca_scatter\n",
    "\n",
    "app.run_server(mode='inline', port=8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13e091",
   "metadata": {},
   "source": [
    "**Plot 3 and Plot 4 Shown Together**\n",
    "\n",
    "Plot 3 --> Bar plots showing each elements contribution to the variance of a given PC.  \n",
    "Plot 4 --> Bar plot showing the correlation of each elements PC-score with the input elemental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be458798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigenvectors (principal components) in df format with labeled indices and columns\n",
    "eigenvectors_dataStdz_df = pd.DataFrame(eigenvectors_dataStdz, index=pc_cols, columns=elem)\n",
    "# determine the correlation between the pca scores and the standardized data\n",
    "dataStdz_pca_corr = np.corrcoef(dataStdz, dataStdz_scores, rowvar=False)\n",
    "# extract the upper right quadrant of the correlation matrix (could also use lower left)\n",
    "dataStdz_pca_corr = dataStdz_pca_corr[0:len(elem), len(elem)::]\n",
    "# add index and column labels to the correlation matrix\n",
    "dataStdz_pca_corr_df = pd.DataFrame(dataStdz_pca_corr, index=elem, columns=pc_cols)\n",
    "\n",
    "app = JupyterDash(__name__) # initiate the app\n",
    "\n",
    "app.layout = html.Div([\n",
    "        html.Div([\n",
    "                html.H5('Select PC'),\n",
    "                dcc.Dropdown(\n",
    "                    id='pc_dd_bar_ppm_stand',\n",
    "                    options=[{'label':'PC_'+str(i+1), 'value':'pc_'+str(i+1)} for i in range(10)],\n",
    "                    value='pc_1',\n",
    "                    multi=False\n",
    "                    )\n",
    "                ], style={'width':'20%'}),\n",
    "        html.Div(dcc.Graph(id='bar-pca-ppm-stand')),\n",
    "        html.Div(dcc.Graph(id='bar-pca-ppm-corr-stand'))\n",
    "    ])\n",
    "\n",
    "@app.callback(\n",
    "    Output('bar-pca-ppm-stand', 'figure'),\n",
    "    Output('bar-pca-ppm-corr-stand', 'figure'),\n",
    "    [Input('pc_dd_bar_ppm_stand', 'value')])\n",
    "def update_pca_ppm_bar(value): \n",
    "    \n",
    "    bar_pca_ppm ={'data':[\n",
    "    go.Bar(x=eigenvectors_dataStdz_df.columns, y=eigenvectors_dataStdz_df.loc[value, :],\n",
    "               marker={'color':'deeppink'})],\n",
    "    \n",
    "    'layout':go.Layout(title='Normalized Variance Contribution, ' + value.upper(),\n",
    "                       xaxis={'title':'Variables (Elements)'},\n",
    "                       yaxis={'title':'Normalized Variance'})\n",
    "    }\n",
    "    \n",
    "    bar_pca_ppm_corr={'data':[\n",
    "    go.Bar(x=dataStdz_pca_corr_df.index, y=dataStdz_pca_corr_df[value],\n",
    "               marker={'color':'deeppink'})],\n",
    "    \n",
    "    'layout':go.Layout(title='Correlation of select PC vs Elements, '+ value.upper(),\n",
    "                       xaxis={'title':'Variables (Elements)'},\n",
    "                       yaxis={'title':'Correlation Coefficient (-1,+1)'})\n",
    "                        }\n",
    "    \n",
    "    return bar_pca_ppm, bar_pca_ppm_corr\n",
    "\n",
    "app.run_server(mode='inline', port=8090)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f823808",
   "metadata": {},
   "source": [
    "## What does all of this mean?\n",
    "\n",
    "_Performing the PCA took a number of steps, was it all worth it and what does it help to clarify?_\n",
    "* The PCA was being used, in part, as a data exploration method.  Specifically, the hope is to improve understanding of how the various elements relate to each other and to the lithologies.\n",
    "* Interpreting >40 elements is difficult, but doing so using a dimensionality reduction tool like PCA makes this task more manageable.  \n",
    "* The approach was effective for detecting differences between the three primary lithologies.  It was also effective at highlighting the lithology (Black Shale/Pyrite) that holds the highest potential of containing critical minerals.  \n",
    "\n",
    "_What's next?_\n",
    "* This dataset includes only a portion of the data from 3 geographic locations. In total ~20 locations will be sampled and many (hundreds) of additional samples will be available for further PCA.\n",
    "* PCA is just one of many methods for reducing dimensionality.  Other methods, such as k-means analysis, can be helpful for clustering data and clarifying groups.  At the very least one other method will be used (likely k-means) to identify key groupings of elements.  \n",
    "* This study is largely opensource, so suggestions are welcome for other data exploration methods.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
